{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a956426e-a09f-459b-9703-d8e8a65b6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers safetensors langchain chromadb faiss-cpu nltk bitsandbytes pandas sklearn tiktoken sentence-transformers trl peft\n",
    "%pip install torch --ignore-installed\n",
    "%pip install accelerate \n",
    "%pip install transformers datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2b63e6-318b-48a2-83a6-4930837dff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import bfloat16\n",
    "from transformers import AutoTokenizer, AutoModel,pipeline,TrainingArguments\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from IPython.display import Markdown, display\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b72fe9d-bc71-44fc-a375-ad91d30928b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path of the csv file. \n",
    "dataset_path = r\"C:\\Users\\48512\\Desktop\\RAG\\medium.csv\"     # Path here\n",
    "offload_path = r\"C:\\Users\\48512\\Desktop\\RAG\\Offloader\"      # Path here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e66b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d1a9138-72cc-4c39-841a-df141c050c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Load the pre-trained transformer model and its corresponding tokenizer from Hugging Face's Transformers library.\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Check if the tokenizer has a padding token set; if not, add '[PAD]' as the padding token.\n",
    "# This is necessary for models that require input lengths to be uniform.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer)) \n",
    "tokenizer.padding_side = \"right\"\n",
    "model_ = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e0e65b-9b2c-4e0b-a86b-e23c5343cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrameLoader to load data from a DataFrame. \n",
    "titles = DataFrameLoader(df, page_content_column=\"Title\")\n",
    "document = titles.load()\n",
    "\n",
    "# Create a TokenTextSplitter instance configured to split text into chunks.\n",
    "# 'chunk_size' specifies the number of tokens each chunk should contain, and\n",
    "# 'chunk_overlap' specifies the number of tokens that subsequent chunks will overlap.\n",
    "splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=75)\n",
    "splitted_texts = splitter.split_documents(document)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35462f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an embedding model with a specified transformer model from Hugging Face.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a Chroma database from split text documents using the specified embedding model.\n",
    "chroma_database = Chroma.from_documents(splitted_texts,\n",
    "                                      embedding_model,\n",
    "                                      persist_directory = 'chroma_db')\n",
    "\n",
    "retriever = chroma_database.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8d4c41-31aa-436b-be8f-3ec47e73ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(texts, batch_size=128):\n",
    "\n",
    "    all_embeddings = []\n",
    "    # Iterate over the list of texts in batches of size batch_size.\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        with torch.no_grad():# Disable gradients for efficiency.\n",
    "            # Tokenize and pad texts, converting them to PyTorch tensors.\n",
    "            tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            embeddings = model_(**tokens).last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "    # Combine all batch embeddings into a single array and return.\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5cd70d-6517-4880-bf3b-0a4941737f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['Title'].tolist()\n",
    "\n",
    "embeddings = embed_text(texts)\n",
    "# Initialize a PCA object from the scikit-learn library with 128 target components.\n",
    "pca = PCA(n_components=128)\n",
    "# Fit the PCA model to the embeddings and transform them to the reduced space.\n",
    "reduced_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ad3491-598d-49df-bd3a-67f252f7df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of dimensions in the reduced embeddings.\n",
    "dimension = reduced_embeddings.shape[1]\n",
    "# Create a quantizer index for L2 distance. This will be used to partition the dataset.\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "# Create an index using an Inverted File system with the quantizer. This index is optimized for L2 distance, contains 20 partitions, and uses the quantizer for the coarse quantization.\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, 20, faiss.METRIC_L2)\n",
    "# Train the index with the reduced embeddings.\n",
    "index.train(reduced_embeddings)\n",
    "index.add(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca162d5-8957-4e9e-aed1-c54bd5cf89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def search_articles(query, k=3):\n",
    "    # Embed the query text\n",
    "    query_vec = embed_text([query])\n",
    "    # Transform the query embedding to the reduced dimensionality space using PCA.\n",
    "    query_vec = pca.transform(query_vec) \n",
    "     # Perform a search on the FAISS index\n",
    "    distances, indices = index.search(query_vec, k)\n",
    "     # Loop through each found index and distance and Retrieve the title and text of the corresponding article using the index\n",
    "    print(f'Query: {query}')\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        title = df.iloc[idx]['Title']\n",
    "        text = df.iloc[idx]['Text']\n",
    "       # Limit the displayed text  \n",
    "        limited_text = ' '.join(text.split()[:300]) + \"...\"\n",
    "        print(f\"\\nTitle: {title}\\nText: {limited_text}\\nDistance: {dist}\\n\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3878a92-f0c2-464f-8c72-7d79500598aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the impact of climate change on polar bears?\n",
      "\n",
      "Title: Time Series Analysis & Climate Change\n",
      "Text: Wrangling of CO₂ emissions data This section will tackle the wrangling of our Carbon Dioxide emissions data. We will use some of the same techniques used above, as well as looking at some new ones: Slicing and Searching Useful functions Familiar techniques From our DataFrame, we will use only the row representing the CO₂ emissions for the entire world. Like before, we will create a new DataFrame that uses a DateTime index — and then use the raw data to populate it. Creating a DataFrame — and populating it — with world emissions data Resulting emissions DataFrame Slicing and Searching DateTime indexes make for convenient slicing of data, let’s select all of our data after the year 2011: e[e.index.year>2011] Slice of emissions data after the year 2011 (notice the missing data) (CREDIT: Author on Jupyter Notebook) Hmm. There seems to be a few NaN’s towards the end of our data — lets use Panda’s fillna method to deal with this. e.fillna(method='ffill', inplace=True) e[e.index.year>2011] Slice of emissions data after the year 2011 (no missing data) Much better! We can also make use of the DateTime index to search for values within a specific range: e['1984-01-04':'1990-01-06'] Resulting slice of emissions data within the specified range This functionality starts to become very useful with more granular time-based data — in our case we have years, and so a range index would probably have been sufficient. Useful functions Pandas provides a whole range of other functions that can be very useful when dealing with time series data — we cannot cover them all in this tutorial, but some are listed below: DataFrame.rolling → provides rolling window calculations Pandas.to_datetime → a replacement for datetime.datetime’s strptime function, it is more useful as it can infer the format TimSeries.shift & TimSeries.tshift → allows for shifting or lagging...\n",
      "Distance: 24.764026641845703\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Wrangling of CO₂ emissions data\\n\\nThis section will tackle the wrangling of our Carbon Dioxide emissions data. We will use some of the same techniques used above, as well as looking at some new ones:\\n\\nSlicing and Searching\\n\\nUseful functions\\n\\nFamiliar techniques\\n\\nFrom our DataFrame, we will use only the row representing the CO₂ emissions for the entire world. Like before, we will create a new DataFrame that uses a DateTime index — and then use the raw data to populate it.\\n\\nCreating a DataFrame — and populating it — with world emissions data\\n\\nResulting emissions DataFrame\\n\\nSlicing and Searching\\n\\nDateTime indexes make for convenient slicing of data, let’s select all of our data after the year 2011:\\n\\ne[e.index.year>2011]\\n\\nSlice of emissions data after the year 2011 (notice the missing data) (CREDIT: Author on Jupyter Notebook)\\n\\nHmm. There seems to be a few NaN’s towards the end of our data — lets use Panda’s fillna method to deal with this.\\n\\ne.fillna(method='ffill', inplace=True)\\n\\ne[e.index.year>2011]\\n\\nSlice of emissions data after the year 2011 (no missing data)\\n\\nMuch better! We can also make use of the DateTime index to search for values within a specific range:\\n\\ne['1984-01-04':'1990-01-06']\\n\\nResulting slice of emissions data within the specified range\\n\\nThis functionality starts to become very useful with more granular time-based data — in our case we have years, and so a range index would probably have been sufficient.\\n\\nUseful functions\\n\\nPandas provides a whole range of other functions that can be very useful when dealing with time series data — we cannot cover them all in this tutorial, but some are listed below:\\n\\nDataFrame.rolling → provides rolling window calculations\\n\\nPandas.to_datetime → a replacement for datetime.datetime’s strptime function, it is more useful as it can infer the format\\n\\nTimSeries.shift & TimSeries.tshift → allows for shifting or lagging of the values of a time series backward and forwards in time\\n\\nFor more information and functionality, see this great Pandas page on time series.\\n\\nVisualizing\\n\\nNow that we have our datasets nicely wrangled, let’s look at how to plot them. We will be using two plotting libraries, namely:\\n\\nMatplotlib\\n\\nPlotly\\n\\nPlotting with Matplotlib\\n\\nMatplotlib is a very popular 2D plotting library for Python, and can easily be downloaded using pip.\\n\\nLet's plot our temperature data again using Matplotlib, this time we will do it more fancily — adding axis labels and titles, etc.:\\n\\nCode to plot temperature anomalies using Matplotlib\\n\\nResulting temperature plot using Matplotlib (CREDIT: Author on Jupyter Notebook)\\n\\nAnd our CO₂ emissions data:\\n\\nCode to plot emissions using Matplotlib\\n\\nResulting emissions plot using Matplotlib (CREDIT: Author on Jupyter Notebook)\\n\\nPlotting with Plotly\\n\\nPlotly is a great library for generating plots that are both interactive and suitable for the web. The Plotly Python package is an open-source library built on plotly.js — which is in turn built on d3.js. In this tutorial, we will be using a wrapper called cufflinks — this makes it easy to use Plotly with Pandas DataFrames.\\n\\nImporting Plotly and Cufflinks correctly for offline mode\\n\\nNow that we have the library correctly imported, let’s plot both datasets again, this time using Plotly and Cufflinks:\\n\\nPlotting temperature data using Plotly\\n\\nPlotting emissions data using Plotly\\n\\nThe resulting plots look much nicer — and are interactive:\\n\\nResulting temperature plot using Plotly (CREDIT: Author on Jupyter Notebook)\\n\\nResulting emissions plot using Plotly (CREDIT: Author on Jupyter Notebook)\\n\\nTime Series Correlation\\n\\nAlthough it seems relatively obvious that both series are trending upwards, what we’d actually like to do here is determine whether the temperature change is as a result of the CO₂ emissions.\\n\\nGranger Causality\\n\\nNow, proving causation is actually very difficult — and just because two things are correlated, does not mean that one causes the other (as any statistician will earnestly tell you!). What we will do instead is determine how helpful the CO₂ emissions data is in forecasting the temperature data; to do this we will use the Granger Causality test.\\n\\nDo not be fooled by the name, Granger Causality does not test for true causality. It would actually be more apt to call it Granger Predictability, or something along those lines.\\n\\nAnyway — what this test does is determine whether one time series will be useful in forecasting another.\\n\\nDynamic Time Warping\\n\\nWe, humans, have developed a number of clever techniques to help us measure the similarity between time series — Dynamic Time Warping (DTW) is one such technique. What DTW is particularly good at, as measuring similarities between series that ‘differ in speed’.\\n\\nFor example, and to quote Wikipedia:\\n\\n“Similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation.”\\n\\nAs this blog post was running awfully long, I have decided to separate out both the Granger Causality, as well as the Dynamic Time Warping stuff, into a separate post.\\n\\nModeling and Forecasting\\n\\nOkay so we can pull, wrangle and visualize our data — and we can do Granger Causality tests to determine whether one time series can be used to predict another; but what about forecasting?\\n\\nForecasting is fun because it allows us to take a stab at predicting the future.\\n\\nIn this section, we will look at forecasting using a library, namely Facebook’s Prophet library.\\n\\nWe will also briefly look at ARIMA models — though so as to keep this blog post from getting unmanageably long, we will not go through ARIMA in too much detail (at least not in this post).\\n\\nForecasting using Facebook’s Prophet\\n\\nOur blue overlords — Facebook — have released an extremely powerful and user-friendly library named ‘Prophet’. Prophet makes is possible for those with little-to-no experience to predict time series, whilst providing intuitive parameters that are simple to tune. The library works in a similar way to the models in sklearn — an instance of the Prophet is instantiated, and then the fit and predict methods are called. This may come as a breath-of-fresh-air to you machine learning enthusiasts out there, it certainly did to me.\\n\\nCreating, fitting and plotting a model for Temperature\\n\\nWe will first import Prophet, and then create a separate DataFrame into which we will copy the data across in the correct format — Prophet takes a DataFrame with two columns, one for the date and one for the values. The date column must be called ‘ds’ whilst the value column must be called ‘y’. You could do this by modifying your original DataFrame, but I opted to just create a new one:\\n\\nPython code to train a temperature anomaly model using Prophet\\n\\nResulting forecast for temperature anomalies (CREDIT: Author on Jupyter Notebook)\\n\\nAnd there we have it, a prediction for global temperature anomalies over the next 100 years! Notice the light blue region, which widens as we move further into the future? That is the forecast’s uncertainty; which grows as we proceed further forward in time.\\n\\nRemember that this forecast only takes into account past anomaly data and nothing else. In reality, this could prove problematic as the heat retained may actually increase exponentially with increased CO₂ in the atmosphere. The picture below shows NASA’s current global surface temperature forecast for different levels of emissions into the future (hey, we didn’t do too badly!).\\n\\nNASA’s forecast (CREDIT: Nasa forecast)\\n\\nDecomposition\\n\\nAs was stated at the beginning of this article, it can be useful to think of a time series as being made up of several components; and luckily for us, Prophet can help us break up our model into these components — so that we can see them visually.\\n\\nWe have already instantiated our model above, to split the model up into its components — run the following code:\\n\\n# Plot the forecast components\\n\\nm.plot_components(forecast);\\n\\nBy default, you’ll see the trend and yearly seasonality of the time series. If you include holidays, you’ll see those here, too. (With more granular data — the weekly seasonality will be shown too).\\n\\n(CREDIT: FB Prophet, and author on Jupyter Notebook)\\n\\nWhat can you tell by looking at the components? Leave your comments below.\\n\\nForecasting using ARIMA\\n\\nAutoregressive Integrated Moving Average — or ARIMA — is a forecasting technique which is able to project future values of a series.\\n\\nARIMA is part of a broader class of time series models, all of which are very useful in that they provide a means through which we can use linear regression type models on non-stationary data.\\n\\nStationarity basically means that your data is not evolving with time (see explanation in the next section). Linear models require stationarity; they are good at dealing with and predicting stationary data.\\n\\nSo the basic intuition is that we’d like to achieve a stationary time series that we can do linear regression on, and ARIMA is just linear regression with some terms which ‘force’ your time series to be stationary.\\n\\nAs this blog post is getting rather long — I have decided to leave Autoregressive Integrated Moving Average Modeling for another day, and another post. For now, see this post for a great introduction to the various forecasting methods (including ARIMA).\\n\\nGeneral Tips, Terms, and Common Pitfalls\\n\\nTerms\\n\\nAutocorrelation\\n\\nAutocorrelation is an important concept to understand when doing time series analyses; the term refers to (and is a mathematical representation of) the degree of similarity between a given time series, and a lagged version of itself over successive time intervals. Think of autocorrelation as the correlation of a time series with itself — it is thus sometimes referred to as lagged correlation (or serial correlation). If you are interested in doing ARIMA modeling (see below) — an understanding of autocorrelation is doubly important.\\n\\nSpurious Correlation\\n\\nSpurious correlations are actually not-altogether uncommon phenomena in statistics; a spurious correlation is a mathematical relationship in which two or more events or variables are associated but not causally related. This can be due to either coincidence or the presence of a third, unseen factor (sometimes called a “common response variable”, “confounding factor”, or “lurking variable”).\\n\\nStationarity\\n\\nA stationary time series is one in which several statistical properties — namely the mean, variance, and covariance — do not vary with time. This means that, although the values can change with time, the way the series itself changes with time does not change over time.\\n\\nStationary vs. Non-Stationary Time Series (CREDIT: Author on Canva)\\n\\nFor more on this, check here and here. We will not dive too deep into stationarity — but we will do go over a how we can test for stationarity, and how we can make our two series stationary (for the purpose of the Granger Causality test) in this post.\\n\\nTips\\n\\nCorrelation is not Causation\\n\\nWhat has come to be a basic mantra in the world of statistics is that correlation does not equal causation. This means that just because two things appear to be related to one another does not mean that one causes the other. This is a worthwhile lesson to learn early on.\\n\\nCorrelation does not have to equal causation (Credit: original)\\n\\nBeware of trend\\n\\nTrends occur in many time series, and before embarking on an exploration of the relationship between two different time series, you should first attempt to measure and control for this trend. In doing so, you will lessen the chance of encountering spurious correlations. But even de-trending a time series cannot protect you from all spurious correlations — patterns such as seasonality, periodicity and autocorrelation can too.\\n\\nBe aware of how you deal with a trend\\n\\nIt is possible to de-trend naively. Attempting to achieve stationarity using (for example) a first differences approach may spoil your data if you are looking for lagged effects.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the impact of climate change on polar bears?\"\n",
    "search_articles(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b3fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48512\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_embedding = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "documents = texts\n",
    "doc_embeddings = model_embedding.encode(documents)\n",
    "pca = PCA(n_components=5)\n",
    "reduced_embeddings = pca.fit_transform(doc_embeddings)\n",
    "index = faiss.IndexFlatL2(5)\n",
    "index.add(reduced_embeddings.astype(np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16a23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, k=1):\n",
    "    query_embedding = model_embedding.encode([query])\n",
    "    query_embedding = pca.transform(query_embedding)\n",
    "    _, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "    return \" \".join(documents[i] for i in indices[0])\n",
    "\n",
    "def generate_response(query):\n",
    "    context = retrieve_context(query)\n",
    "    input_ids = tokenizer_gpt.encode(context + query, return_tensors=\"pt\")\n",
    "    generated_ids = model_gpt.generate( input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=150,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3)\n",
    "    response = tokenizer_gpt.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccb5ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Things Every Aspiring Data Scientist Should Know Before Their First JobWhat is the Data Science? This article examines data science careers and provides the general guidelines and details on data science jobs and careers for data scientists and their organizations.1\n",
      "\n",
      "Research and analysis: A Data Scientist's Career The first part of this article is about the research and analysis required by the study of data science. 2 Then there is the following points: How will this data science career be different from other career fields and how will this be affected by different disciplines? How is the study about data science related to your field/industry relevant to you? Does the methodology for data science related to this career matter to you? Finally, if you are not familiar with the different data\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the Data Science?\"\n",
    "answer = generate_response(question)\n",
    "print( answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d78a18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
